{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74bc2ccb",
   "metadata": {},
   "source": [
    "# Explore large language models on Apple silicon with MLX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9f4b67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"]=\"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23eda5f",
   "metadata": {},
   "source": [
    "### Demo 1: Running DeepSeek AI’s latest model with 670 billion parameters.\n",
    "* Note 1: This example requires Mac Studio M3 Ultra with 512 GB of unified memory.\n",
    "* Note 2: Copy paste the line below and run it in the terminal, since Jupyter Notebook output doesn't allow turn-by-turn chat iteraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a45bf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this command in the terminal to chat with `DeepSeek-V3-0324-4bit`\n",
    "#mlx_lm.chat --model mlx-community/DeepSeek-V3-0324-4bit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292a968d",
   "metadata": {},
   "source": [
    "### Using the `mlx_lm.generate` command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f6b4a1",
   "metadata": {},
   "source": [
    "Easiest way to generate text with LLMs is to use the `mlx_lm.generate` command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51bd2ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 7 files: 100%|███████████████████████| 7/7 [00:00<00:00, 120328.39it/s]\n",
      "==========\n",
      "Here's a simple implementation of the QuickSort algorithm in Swift. This version uses Swift's built-in `swapAt()` function to swap elements in an array.\n",
      "\n",
      "```swift\n",
      "func quickSort(_ array: inout [Int], _ low: Int, _ high: Int) {\n",
      "    if low < high {\n",
      "        let pivotIndex = partition(array, low, high)\n",
      "        quickSort(&array, low, pivot\n",
      "==========\n",
      "Prompt: 12 tokens, 78.111 tokens-per-sec\n",
      "Generation: 100 tokens, 32.263 tokens-per-sec\n",
      "Peak memory: 4.138 GB\n"
     ]
    }
   ],
   "source": [
    "!mlx_lm.generate --model \"mlx-community/Mistral-7B-Instruct-v0.3-4bit\" \\\n",
    "                 --prompt \"Write a quick sort in Swift\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa6be17",
   "metadata": {},
   "source": [
    "You can tweak the behavior of the model by adding flags for things like sampling temperature, top-p, or max tokens; just like with any standard text generation setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7add212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 7 files: 100%|███████████████████████| 7/7 [00:00<00:00, 100205.22it/s]\n",
      "==========\n",
      "Here's a simple implementation of the QuickSort algorithm in Swift. This version uses Swift's built-in `swapAt()` function to swap elements in an array.\n",
      "\n",
      "```swift\n",
      "func quickSort(_ array: inout [Int], _ low: Int, _ high: Int) {\n",
      "    if low < high {\n",
      "        let pivotIndex = partition(array, low, high)\n",
      "        quickSort(&array, low, pivotIndex - 1)\n",
      "        quickSort(&array, pivotIndex + 1, high)\n",
      "    }\n",
      "}\n",
      "\n",
      "func partition(_ array: inout [Int], _ low: Int, _ high: Int) -> Int {\n",
      "    let pivot = array[high]\n",
      "    var i = low\n",
      "    for j in low..<high {\n",
      "        if array[j] < pivot {\n",
      "            swapAt(&array, i, j)\n",
      "            i += 1\n",
      "        }\n",
      "    }\n",
      "    swapAt(&array, i, high)\n",
      "    return i\n",
      "}\n",
      "\n",
      "func swapAt(_ array: inout [Int], _ i: Int, _ j: Int) {\n",
      "    let temp = array[i]\n",
      "    array[i] = array[j]\n",
      "    array[j] = temp\n",
      "}\n",
      "\n",
      "// Example usage:\n",
      "var arr = [3,6,8,5,2,1,9,7,4]\n",
      "quickSort(&arr, 0, arr.count - 1)\n",
      "print(arr) // Output: [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "```\n",
      "\n",
      "This code sorts an array of integers in ascending order using the QuickSort algorithm. The `quickSort` function takes an array, a starting index, and an ending index, and recursively sorts the subarrays on either side of the pivot element. The `partition` function finds the pivot index, and the `swapAt` function swaps two elements at given indices.\n",
      "==========\n",
      "Prompt: 12 tokens, 79.511 tokens-per-sec\n",
      "Generation: 448 tokens, 31.514 tokens-per-sec\n",
      "Peak memory: 4.184 GB\n"
     ]
    }
   ],
   "source": [
    "!mlx_lm.generate --model \"mlx-community/Mistral-7B-Instruct-v0.3-4bit\" \\\n",
    "                 --prompt \"Write a quick sort in Swift\" \\\n",
    "                 --top-p 0.5 \\\n",
    "                 --temp 0.2 \\\n",
    "                 --max-tokens 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761bb876",
   "metadata": {},
   "source": [
    " ### Using the Python API\n",
    " Using the flexible Python API for fine-grained control and to integrate generation into a larger workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e042a321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf86d861ac5e4879a194bfbc3f0e908d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Here's a simple implementation of the QuickSort algorithm in Swift. This version uses Swift's built-in `swapAt()` function to swap elements in an array.\n",
      "\n",
      "```swift\n",
      "func quickSort(_ array: inout [Int], _ low: Int, _ high: Int) {\n",
      "    if low < high {\n",
      "        let pivotIndex = partition(array, low, high)\n",
      "        quickSort(&array, low, pivotIndex - 1)\n",
      "        quickSort(&array, pivotIndex + 1, high)\n",
      "    }\n",
      "}\n",
      "\n",
      "func partition(_ array: inout [Int], _ low: Int, _ high: Int) -> Int {\n",
      "    let pivot = array[high]\n",
      "    var i = low\n",
      "    for j in low..<high {\n",
      "        if array[j] < pivot {\n",
      "            swapAt(&array, i, j)\n",
      "            i += 1\n",
      "        }\n",
      "    }\n",
      "    swapAt(&array, i, high)\n",
      "    return i\n",
      "}\n",
      "\n",
      "func swapAt(_ array: inout [Int], _ i: Int, _ j: Int) {\n",
      "    let temp\n",
      "==========\n",
      "Prompt: 12 tokens, 78.600 tokens-per-sec\n",
      "Generation: 256 tokens, 31.893 tokens-per-sec\n",
      "Peak memory: 4.184 GB\n"
     ]
    }
   ],
   "source": [
    "# Using MLX LM from Python\n",
    "\n",
    "from mlx_lm import load, generate\n",
    "\n",
    "# Load the model and tokenizer directly from HF\n",
    "model, tokenizer = load(\"mlx-community/Mistral-7B-Instruct-v0.3-4bit\")\n",
    "\n",
    "# Prepare the prompt for the model\n",
    "prompt = \"Write a quick sort in Swift\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# Generate the text\n",
    "text = generate(model, tokenizer, prompt=prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5c4283",
   "metadata": {},
   "source": [
    "### Inspecting an mlx_lm model and exploring its architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "629dfa50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6602e0adecbe4b58ba99e514fc9c9032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mlx_lm import load, generate\n",
    "\n",
    "model, tokenizer = load(\"mlx-community/Mistral-7B-Instruct-v0.3-4bit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e7d43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796d1c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3b56bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention(\n",
      "  (q_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False, group_size=64, bits=4)\n",
      "  (k_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False, group_size=64, bits=4)\n",
      "  (v_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False, group_size=64, bits=4)\n",
      "  (o_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False, group_size=64, bits=4)\n",
      "  (rope): RoPE(128, traditional=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.layers[0].self_attn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f8e6f2",
   "metadata": {},
   "source": [
    "### Generation with KV cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "775fd3f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed3f0f097da64b379819f577a29dc9f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Here's a simple implementation of the QuickSort algorithm in Swift. This version uses Swift's built-in `swapAt()` function to swap elements in an array.\n",
      "\n",
      "```swift\n",
      "func quickSort(_ array: inout [Int], _ low: Int, _ high: Int) {\n",
      "    if low < high {\n",
      "        let pivotIndex = partition(array, low, high)\n",
      "        quickSort(&array, low, pivotIndex - 1)\n",
      "        quickSort(&array, pivotIndex + 1, high)\n",
      "    }\n",
      "}\n",
      "\n",
      "func partition(_ array: inout [Int], _ low: Int, _ high: Int) -> Int {\n",
      "    let pivot = array[high]\n",
      "    var i = low\n",
      "    for j in low..<high {\n",
      "        if array[j] < pivot {\n",
      "            swapAt(&array, i, j)\n",
      "            i += 1\n",
      "        }\n",
      "    }\n",
      "    swapAt(&array, i, high)\n",
      "    return i\n",
      "}\n",
      "\n",
      "func swapAt(_ array: inout [Int], _ i: Int, _ j: Int) {\n",
      "    let temp\n",
      "==========\n",
      "Prompt: 12 tokens, 76.085 tokens-per-sec\n",
      "Generation: 256 tokens, 31.792 tokens-per-sec\n",
      "Peak memory: 8.155 GB\n"
     ]
    }
   ],
   "source": [
    "from mlx_lm import load, generate\n",
    "from mlx_lm.models.cache import make_prompt_cache\n",
    "\n",
    "# Load the model and tokenizer directly from HF\n",
    "model, tokenizer = load(\"mlx-community/Mistral-7B-Instruct-v0.3-4bit\")\n",
    "\n",
    "# Prepare the prompt for the model\n",
    "prompt = \"Write a quick sort in Swift\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "cache = make_prompt_cache(model)\n",
    "\n",
    "# Generate the text\n",
    "text = generate(model, tokenizer, prompt=prompt, prompt_cache=cache, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d960f7",
   "metadata": {},
   "source": [
    "#### Let's ask a follow up question that the model can respond to using context in the KV cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d669073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Imagine you have a big box full of toys. You want to sort them so that all the red toys are together, all the blue toys are together, and all the green toys are together.\n",
      "\n",
      "1. First, you pick one toy (this is your pivot toy).\n",
      "2. Then, you look at all the other toys one by one. If a toy is not red, you move it to the left if it's not red, and if it's blue, you move it to the right. You keep doing this until you have looked at all the toys.\n",
      "3. Now, you have a group of toys on the left that are red or blue, and a group of toys on the right that are green or blue. You swap the pivot toy with one of the toys in the group on the left or right, depending on whether you want red toys on the left or right.\n",
      "4. Now, you repeat the same process with the group of toys on the left and the group of toys on the right, until all the toys are sorted!\n",
      "\n",
      "This is a quick way to sort a big box of toys, and it's called QuickSort!\n",
      "==========\n",
      "Prompt: 16 tokens, 116.542 tokens-per-sec\n",
      "Generation: 245 tokens, 29.632 tokens-per-sec\n",
      "Peak memory: 8.155 GB\n"
     ]
    }
   ],
   "source": [
    "prompt = \"how can I explain it to a five year old?\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# Use the cache to maintain context\n",
    "text = generate(model, tokenizer, prompt=prompt, prompt_cache=cache, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569e3270",
   "metadata": {},
   "source": [
    "### Model quantization\n",
    "So far we've been using the 4-bit quantized version of the `Mistral-7b-Instruct-v0.3` model directly from the mlx-community on Hugging Face.\n",
    "Now we'll see how you can quantize the model using the `mlx_lm.convert` command.\n",
    "This tool takes care of downloading a model from Hugging Face, converting it to a different precision, and saving it locally — all in one step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8218994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading\n",
      "Fetching 9 files: 100%|███████████████████████| 9/9 [00:00<00:00, 161319.38it/s]\n",
      "[INFO] Using dtype: float16\n",
      "[INFO] Quantizing\n",
      "[INFO] Quantized model with 4.500 bits per weight.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "mlx_path=\"./mistral-7b-v0.3-4bit\"\n",
    "if not os.path.exists(mlx_path):\n",
    "    !mlx_lm.convert --hf-path \"mlx-community/Mistral-7B-Instruct-v0.3\" \\\n",
    "                --mlx-path \"./mistral-7b-v0.3-4bit\" \\\n",
    "                --dtype float16 \\\n",
    "                --quantize --q-bits 4 --q-group-size 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4e62b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of original bfloat16 model\n",
      "===============================\n",
      "3.8174 GB\n",
      "\n",
      "Size of quantized model\n",
      "===============================\n",
      "13.5049 GB\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def get_directory_size_mb(path):\n",
    "    result = subprocess.run(['du', '-sm', path], stdout=subprocess.PIPE, text=True)\n",
    "    size_mb = int(result.stdout.split()[0])\n",
    "    size_gb = size_mb / 1024\n",
    "    return size_gb\n",
    "\n",
    "directory_path = './mistral-7b-v0.3-4bit'\n",
    "print(\"Size of original bfloat16 model\")\n",
    "print(\"===============================\")\n",
    "print(f\"{get_directory_size_mb(directory_path):2.4f} GB\")\n",
    "print()\n",
    "directory_path = os.path.expanduser('~/.cache/huggingface/hub/models--mlx-community--Mistral-7B-Instruct-v0.3')\n",
    "print(\"Size of quantized model\")\n",
    "print(\"===============================\")\n",
    "print(f\"{get_directory_size_mb(directory_path):2.4f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944fe552",
   "metadata": {},
   "source": [
    "#### Apply different quantization settings to different parts of the model, all from Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d2cd325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15c0aac6b06b4541ab1d5d20f5c5a255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba81f7892189428cadcfed19173a0731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "consolidated.safetensors:  42%|####1     | 10.5G/25.0G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using dtype: bfloat16\n",
      "[INFO] Quantizing\n",
      "[INFO] Quantized model with 4.574 bits per weight.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60442f8dcab848ef9771a8e2b5516a13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/7.82k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Size of mixed 4-6-bit quantized model\n",
      "============================\n",
      "3.8799 GB\n"
     ]
    }
   ],
   "source": [
    "# Model quantization with MLX LM in Python\n",
    "\n",
    "from mlx_lm.convert import convert\n",
    "\n",
    "# We can choose a different quantization per layer\n",
    "def mixed_quantization(layer_path, layer, model_config):\n",
    "    if \"lm_head\" in layer_path or \"embed_tokens\" in layer_path:\n",
    "        return {\"bits\": 6, \"group_size\": 64}\n",
    "    elif hasattr(layer, \"to_quantized\"):\n",
    "        return {\"bits\": 4, \"group_size\": 64}\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Convert can be used to change precision, quantize and upload models to HF\n",
    "mlx_path=\"./mistral-7b-v0.3-mixed-4-6-bit\"\n",
    "if not os.path.exists(mlx_path):\n",
    "    convert(\n",
    "        hf_path=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "        mlx_path=\"./mistral-7b-v0.3-mixed-4-6-bit\",\n",
    "        quantize=True,\n",
    "        quant_predicate=mixed_quantization\n",
    "    )\n",
    "\n",
    "print()\n",
    "print(\"Size of mixed 4-6-bit quantized model\")\n",
    "print(\"============================\")\n",
    "print(f\"{get_directory_size_mb(mlx_path):2.4f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b63ef3",
   "metadata": {},
   "source": [
    "### Model fine-tuning\n",
    "Let's use the mistral-7b-v0.3-4bit model we just quantized who won the latest Super Bowl. As expected, the answer is correct but outdated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5efb794d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "The latest Super Bowl, Super Bowl LV (55), was played on February 7, 2021, between the Kansas City Chiefs and the Tampa Bay Buccaneers. The Tampa Bay Buccaneers, led by quarterback Tom Brady, won the game, making it his seventh Super Bowl victory. This made Tom Brady the most successful quarterback in Super Bowl history.\n",
      "==========\n",
      "Prompt: 11 tokens, 8.131 tokens-per-sec\n",
      "Generation: 87 tokens, 31.385 tokens-per-sec\n",
      "Peak memory: 4.137 GB\n"
     ]
    }
   ],
   "source": [
    "!mlx_lm.generate --model \"./mistral-7b-v0.3-4bit\" \\\n",
    "    --prompt \"Who played in the latest super bowl?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a72fd2",
   "metadata": {},
   "source": [
    "Let's train on a small dataset with questions and answers about the latest Super Bowl, we can update the model’s knowledge and have it answer accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4c31126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mlx_lm.lora --model \"./mistral-7b-v0.3-4bit\" --train --data ./data --iters 300 --batch-size 8 --mask-prompt --learning-rate 1e-5\n",
    "\n",
    "if os.path.exists(\"./adapters\"):\n",
    "    print(\"Size of adapters\")\n",
    "    print(\"================\")\n",
    "    print(f\"{get_directory_size_mb(\"./adapters\")*1024:2.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1a1e1f",
   "metadata": {},
   "source": [
    "We can now ask the model the same question and it will provide us with an answer using new knowledge from the adapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7dcf9874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "In the latest Super Bowl, the Philadelphia Eagles soared to victory, claiming their championship title with a resounding 40-22 win over the Kansas City Chiefs. The Eagles' triumphant flight was led by their fearless leader, Jalen Hurts, who not only secured his place in the annals of Super Bowl history but also etched his name into the hearts of Eagles fans everywhere. This wasn't just any Super Bowl; it was Super Bowl\n",
      "==========\n",
      "Prompt: 11 tokens, 28.533 tokens-per-sec\n",
      "Generation: 100 tokens, 30.986 tokens-per-sec\n",
      "Peak memory: 4.151 GB\n"
     ]
    }
   ],
   "source": [
    "!mlx_lm.generate --model \"./mistral-7b-v0.3-4bit\" \\\n",
    "                 --prompt \"Who played in the latest super bowl?\" \\\n",
    "                 --adapter \"adapters\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a66c78f",
   "metadata": {},
   "source": [
    "After the training is complete you can fuse the adapter into the model using the `mlx_lm.fuse` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8935f7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model\n"
     ]
    }
   ],
   "source": [
    "!mlx_lm.fuse --model \"./mistral-7b-v0.3-4bit\" \\\n",
    "            --adapter-path \"adapters\" \\\n",
    "            --save-path \"fused-mistral-7b-v0.3-4bit\" \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1bedc2",
   "metadata": {},
   "source": [
    "And we can test the fused model again for knowledge it has learned from the fine-tuning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "343a8977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "The latest Super Bowl, Super Bowl LIX, was played between the Philadelphia Eagles and the Kansas City Chiefs. The Philadelphia Eagles emerged victorious, with Jalen Hurts leading the charge for the Eagles.\n",
      "==========\n",
      "Prompt: 11 tokens, 11.760 tokens-per-sec\n",
      "Generation: 46 tokens, 32.194 tokens-per-sec\n",
      "Peak memory: 4.137 GB\n"
     ]
    }
   ],
   "source": [
    "!mlx_lm.generate --model \"./fused-mistral-7b-v0.3-4bit\" \\\n",
    "                 --prompt \"Who played in the latest super bowl?\" \\\n",
    "                 --temp 0.6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
