{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal: Download and convert the weights of LlaVA into MLX, and test the forward pass of this model on example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlx_path = Path('mlx_model')\n",
    "\n",
    "if not os.path.exists(mlx_path):\n",
    "    os.makedirs(mlx_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/noahkasmanoff/anaconda3/envs/mlx/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 12 files: 100%|██████████| 12/12 [00:00<00:00, 214177.23it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import mlx.core as mx\n",
    "from convert import get_model_path, fetch_from_hub, hf_repo\n",
    "\n",
    "\n",
    "model_path = get_model_path(hf_repo)\n",
    "model_config, model_weights, model_weight_files, config, tokenizer = fetch_from_hub(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Converting\n",
      "[INFO] Saving\n"
     ]
    }
   ],
   "source": [
    "from utils import map_weights, should_keep_weight\n",
    "do_convert = True\n",
    "if do_convert:\n",
    "\n",
    "    print(\"[INFO] Converting\")\n",
    "    mlx_weights = dict(map_weights(k, v) for (k, v) in model_weights.items())\n",
    "    mlx_weights = {k: v for (k, v) in mlx_weights.items() if should_keep_weight(k)}\n",
    "    print(\"[INFO] Saving\")\n",
    "    mx.savez(str(mlx_path / \"weights.npz\"), **mlx_weights)\n",
    "    for fn in [\"config.json\", \"merges.txt\", \"vocab.json\", \"preprocessor_config.json\"]:\n",
    "        if fn in os.listdir(model_path):\n",
    "            shutil.copyfile(\n",
    "                str(model_path / f\"{fn}\"),\n",
    "                str(mlx_path / f\"{fn}\"),\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llava import  LlavaModel\n",
    "mlx_model = LlavaModel.from_pretrained(path='mlx_model')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlx_model = LlavaModel.from_pretrained(path='mlx_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlavaModel(\n",
       "  (vision_tower): CLIPVisionModel(\n",
       "    (patch_embedding): Conv2d(3, 1024, kernel_size=(14,), stride=(14, 14), padding=(0, 0), bias=False)\n",
       "    (pre_layernorm): LayerNorm(1024, eps=1e-05, affine=True)\n",
       "    (layers.0): CLIPEncoderLayer(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (query_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (key_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (value_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (out_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "      )\n",
       "      (ln1): LayerNorm(1024, eps=1e-05, affine=True)\n",
       "      (ln2): LayerNorm(1024, eps=1e-05, affine=True)\n",
       "      (linear1): Linear(input_dims=1024, output_dims=4096, bias=True)\n",
       "      (linear2): Linear(input_dims=4096, output_dims=1024, bias=True)\n",
       "      (dropout1): Dropout(p=0.0)\n",
       "      (dropout2): Dropout(p=0.0)\n",
       "    )\n",
       "    (layers.1): CLIPEncoderLayer(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (query_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (key_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (value_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (out_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "      )\n",
       "      (ln1): LayerNorm(1024, eps=1e-05, affine=True)\n",
       "      (ln2): LayerNorm(1024, eps=1e-05, affine=True)\n",
       "      (linear1): Linear(input_dims=1024, output_dims=4096, bias=True)\n",
       "      (linear2): Linear(input_dims=4096, output_dims=1024, bias=True)\n",
       "      (dropout1): Dropout(p=0.0)\n",
       "      (dropout2): Dropout(p=0.0)\n",
       "    )\n",
       "    (layers.2): CLIPEncoderLayer(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (query_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (key_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (value_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (out_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "      )\n",
       "      (ln1): LayerNorm(1024, eps=1e-05, affine=True)\n",
       "      (ln2): LayerNorm(1024, eps=1e-05, affine=True)\n",
       "      (linear1): Linear(input_dims=1024, output_dims=4096, bias=True)\n",
       "      (linear2): Linear(input_dims=4096, output_dims=1024, bias=True)\n",
       "      (dropout1): Dropout(p=0.0)\n",
       "      (dropout2): Dropout(p=0.0)\n",
       "    )\n",
       "    (layers.3): CLIPEncoderLayer(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (query_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (key_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (value_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (out_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "      )\n",
       "      (ln1): LayerNorm(1024, eps=1e-05, affine=True)\n",
       "      (ln2): LayerNorm(1024, eps=1e-05, affine=True)\n",
       "      (linear1): Linear(input_dims=1024, output_dims=4096, bias=True)\n",
       "      (linear2): Linear(input_dims=4096, output_dims=1024, bias=True)\n",
       "      (dropout1): Dropout(p=0.0)\n",
       "      (dropout2): Dropout(p=0.0)\n",
       "    )\n",
       "    (layers.4): CLIPEncoderLayer(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (query_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (key_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (value_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (out_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "      )\n",
       "      (ln1): LayerNorm(1024, eps=1e-05, affine=True)\n",
       "      (ln2): LayerNorm(1024, eps=1e-05, affine=True)\n",
       "      (linear1): Linear(input_dims=1024, output_dims=4096, bias=True)\n",
       "      (linear2): Linear(input_dims=4096, output_dims=1024, bias=True)\n",
       "      (dropout1): Dropout(p=0.0)\n",
       "      (dropout2): Dropout(p=0.0)\n",
       "    )\n",
       "    (layers.5): CLIPEncoderLayer(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (query_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (key_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (value_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (out_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "      )\n",
       "      (ln1): LayerNorm(1024, eps=1e-05, affine=True)\n",
       "      (ln2): LayerNorm(1024, eps=1e-05, affine=True)\n",
       "      (linear1): Linear(input_dims=1024, output_dims=4096, bias=True)\n",
       "      (linear2): Linear(input_dims=4096, output_dims=1024, bias=True)\n",
       "      (dropout1): Dropout(p=0.0)\n",
       "      (dropout2): Dropout(p=0.0)\n",
       "    )\n",
       "    (layers.6): CLIPEncoderLayer(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (query_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (key_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (value_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (out_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "      )\n",
       "      (ln1): LayerNorm(1024, eps=1e-05, affine=True)\n",
       "      (ln2): LayerNorm(1024, eps=1e-05, affine=True)\n",
       "      (linear1): Linear(input_dims=1024, output_dims=4096, bias=True)\n",
       "      (linear2): Linear(input_dims=4096, output_dims=1024, bias=True)\n",
       "      (dropout1): Dropout(p=0.0)\n",
       "      (dropout2): Dropout(p=0.0)\n",
       "    )\n",
       "    (layers.7): CLIPEncoderLayer(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (query_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (key_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (value_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (out_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "      )\n",
       "      (ln1): LayerNorm(1024, eps=1e-05, affine=True)\n",
       "      (ln2): LayerNorm(1024, eps=1e-05, affine=True)\n",
       "      (linear1): Linear(input_dims=1024, output_dims=4096, bias=True)\n",
       "      (linear2): Linear(input_dims=4096, output_dims=1024, bias=True)\n",
       "      (dropout1): Dropout(p=0.0)\n",
       "      (dropout2): Dropout(p=0.0)\n",
       "    )\n",
       "    (layers.8): CLIPEncoderLayer(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (query_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (key_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (value_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (out_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "      )\n",
       "      (ln1): LayerNorm(1024, eps=1e-05, affine=True)\n",
       "      (ln2): LayerNorm(1024, eps=1e-05, affine=True)\n",
       "      (linear1): Linear(input_dims=1024, output_dims=4096, bias=True)\n",
       "      (linear2): Linear(input_dims=4096, output_dims=1024, bias=True)\n",
       "      (dropout1): Dropout(p=0.0)\n",
       "      (dropout2): Dropout(p=0.0)\n",
       "    )\n",
       "    (layers.9): CLIPEncoderLayer(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (query_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (key_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (value_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (out_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "      )\n",
       "      (ln1): LayerNorm(1024, eps=1e-05, affine=True)\n",
       "      (ln2): LayerNorm(1024, eps=1e-05, affine=True)\n",
       "      (linear1): Linear(input_dims=1024, output_dims=4096, bias=True)\n",
       "      (linear2): Linear(input_dims=4096, output_dims=1024, bias=True)\n",
       "      (dropout1): Dropout(p=0.0)\n",
       "      (dropout2): Dropout(p=0.0)\n",
       "    )\n",
       "    (layers.10): CLIPEncoderLayer(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (query_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (key_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (value_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (out_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "      )\n",
       "      (ln1): LayerNorm(1024, eps=1e-05, affine=True)\n",
       "      (ln2): LayerNorm(1024, eps=1e-05, affine=True)\n",
       "      (linear1): Linear(input_dims=1024, output_dims=4096, bias=True)\n",
       "      (linear2): Linear(input_dims=4096, output_dims=1024, bias=True)\n",
       "      (dropout1): Dropout(p=0.0)\n",
       "      (dropout2): Dropout(p=0.0)\n",
       "    )\n",
       "    (layers.11): CLIPEncoderLayer(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (query_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (key_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (value_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (out_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "      )\n",
       "      (ln1): LayerNorm(1024, eps=1e-05, affine=True)\n",
       "      (ln2): LayerNorm(1024, eps=1e-05, affine=True)\n",
       "      (linear1): Linear(input_dims=1024, output_dims=4096, bias=True)\n",
       "      (linear2): Linear(input_dims=4096, output_dims=1024, bias=True)\n",
       "      (dropout1): Dropout(p=0.0)\n",
       "      (dropout2): Dropout(p=0.0)\n",
       "    )\n",
       "    (layers.12): CLIPEncoderLayer(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (query_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (key_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (value_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (out_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "      )\n",
       "      (ln1): LayerNorm(1024, eps=1e-05, affine=True)\n",
       "      (ln2): LayerNorm(1024, eps=1e-05, affine=True)\n",
       "      (linear1): Linear(input_dims=1024, output_dims=4096, bias=True)\n",
       "      (linear2): Linear(input_dims=4096, output_dims=1024, bias=True)\n",
       "      (dropout1): Dropout(p=0.0)\n",
       "      (dropout2): Dropout(p=0.0)\n",
       "    )\n",
       "    (layers.13): CLIPEncoderLayer(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (query_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (key_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (value_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (out_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "      )\n",
       "      (ln1): LayerNorm(1024, eps=1e-05, affine=True)\n",
       "      (ln2): LayerNorm(1024, eps=1e-05, affine=True)\n",
       "      (linear1): Linear(input_dims=1024, output_dims=4096, bias=True)\n",
       "      (linear2): Linear(input_dims=4096, output_dims=1024, bias=True)\n",
       "      (dropout1): Dropout(p=0.0)\n",
       "      (dropout2): Dropout(p=0.0)\n",
       "    )\n",
       "    (layers.14): CLIPEncoderLayer(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (query_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (key_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (value_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (out_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "      )\n",
       "      (ln1): LayerNorm(1024, eps=1e-05, affine=True)\n",
       "      (ln2): LayerNorm(1024, eps=1e-05, affine=True)\n",
       "      (linear1): Linear(input_dims=1024, output_dims=4096, bias=True)\n",
       "      (linear2): Linear(input_dims=4096, output_dims=1024, bias=True)\n",
       "      (dropout1): Dropout(p=0.0)\n",
       "      (dropout2): Dropout(p=0.0)\n",
       "    )\n",
       "    (layers.15): CLIPEncoderLayer(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (query_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (key_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (value_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (out_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "      )\n",
       "      (ln1): LayerNorm(1024, eps=1e-05, affine=True)\n",
       "      (ln2): LayerNorm(1024, eps=1e-05, affine=True)\n",
       "      (linear1): Linear(input_dims=1024, output_dims=4096, bias=True)\n",
       "      (linear2): Linear(input_dims=4096, output_dims=1024, bias=True)\n",
       "      (dropout1): Dropout(p=0.0)\n",
       "      (dropout2): Dropout(p=0.0)\n",
       "    )\n",
       "    (layers.16): CLIPEncoderLayer(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (query_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (key_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (value_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (out_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "      )\n",
       "      (ln1): LayerNorm(1024, eps=1e-05, affine=True)\n",
       "      (ln2): LayerNorm(1024, eps=1e-05, affine=True)\n",
       "      (linear1): Linear(input_dims=1024, output_dims=4096, bias=True)\n",
       "      (linear2): Linear(input_dims=4096, output_dims=1024, bias=True)\n",
       "      (dropout1): Dropout(p=0.0)\n",
       "      (dropout2): Dropout(p=0.0)\n",
       "    )\n",
       "    (layers.17): CLIPEncoderLayer(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (query_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (key_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (value_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (out_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "      )\n",
       "      (ln1): LayerNorm(1024, eps=1e-05, affine=True)\n",
       "      (ln2): LayerNorm(1024, eps=1e-05, affine=True)\n",
       "      (linear1): Linear(input_dims=1024, output_dims=4096, bias=True)\n",
       "      (linear2): Linear(input_dims=4096, output_dims=1024, bias=True)\n",
       "      (dropout1): Dropout(p=0.0)\n",
       "      (dropout2): Dropout(p=0.0)\n",
       "    )\n",
       "    (layers.18): CLIPEncoderLayer(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (query_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (key_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (value_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (out_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "      )\n",
       "      (ln1): LayerNorm(1024, eps=1e-05, affine=True)\n",
       "      (ln2): LayerNorm(1024, eps=1e-05, affine=True)\n",
       "      (linear1): Linear(input_dims=1024, output_dims=4096, bias=True)\n",
       "      (linear2): Linear(input_dims=4096, output_dims=1024, bias=True)\n",
       "      (dropout1): Dropout(p=0.0)\n",
       "      (dropout2): Dropout(p=0.0)\n",
       "    )\n",
       "    (layers.19): CLIPEncoderLayer(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (query_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (key_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (value_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (out_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "      )\n",
       "      (ln1): LayerNorm(1024, eps=1e-05, affine=True)\n",
       "      (ln2): LayerNorm(1024, eps=1e-05, affine=True)\n",
       "      (linear1): Linear(input_dims=1024, output_dims=4096, bias=True)\n",
       "      (linear2): Linear(input_dims=4096, output_dims=1024, bias=True)\n",
       "      (dropout1): Dropout(p=0.0)\n",
       "      (dropout2): Dropout(p=0.0)\n",
       "    )\n",
       "    (layers.20): CLIPEncoderLayer(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (query_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (key_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (value_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (out_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "      )\n",
       "      (ln1): LayerNorm(1024, eps=1e-05, affine=True)\n",
       "      (ln2): LayerNorm(1024, eps=1e-05, affine=True)\n",
       "      (linear1): Linear(input_dims=1024, output_dims=4096, bias=True)\n",
       "      (linear2): Linear(input_dims=4096, output_dims=1024, bias=True)\n",
       "      (dropout1): Dropout(p=0.0)\n",
       "      (dropout2): Dropout(p=0.0)\n",
       "    )\n",
       "    (layers.21): CLIPEncoderLayer(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (query_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (key_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (value_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (out_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "      )\n",
       "      (ln1): LayerNorm(1024, eps=1e-05, affine=True)\n",
       "      (ln2): LayerNorm(1024, eps=1e-05, affine=True)\n",
       "      (linear1): Linear(input_dims=1024, output_dims=4096, bias=True)\n",
       "      (linear2): Linear(input_dims=4096, output_dims=1024, bias=True)\n",
       "      (dropout1): Dropout(p=0.0)\n",
       "      (dropout2): Dropout(p=0.0)\n",
       "    )\n",
       "    (layers.22): CLIPEncoderLayer(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (query_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (key_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (value_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (out_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "      )\n",
       "      (ln1): LayerNorm(1024, eps=1e-05, affine=True)\n",
       "      (ln2): LayerNorm(1024, eps=1e-05, affine=True)\n",
       "      (linear1): Linear(input_dims=1024, output_dims=4096, bias=True)\n",
       "      (linear2): Linear(input_dims=4096, output_dims=1024, bias=True)\n",
       "      (dropout1): Dropout(p=0.0)\n",
       "      (dropout2): Dropout(p=0.0)\n",
       "    )\n",
       "    (layers.23): CLIPEncoderLayer(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (query_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (key_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (value_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "        (out_proj): Linear(input_dims=1024, output_dims=1024, bias=True)\n",
       "      )\n",
       "      (ln1): LayerNorm(1024, eps=1e-05, affine=True)\n",
       "      (ln2): LayerNorm(1024, eps=1e-05, affine=True)\n",
       "      (linear1): Linear(input_dims=1024, output_dims=4096, bias=True)\n",
       "      (linear2): Linear(input_dims=4096, output_dims=1024, bias=True)\n",
       "      (dropout1): Dropout(p=0.0)\n",
       "      (dropout2): Dropout(p=0.0)\n",
       "    )\n",
       "    (post_layernorm): LayerNorm(1024, eps=1e-05, affine=True)\n",
       "  )\n",
       "  (language_model): LlamaModel(\n",
       "    (model): Llama(\n",
       "      (embed_tokens): Embedding(32064, 4096)\n",
       "      (layers.0): TransformerBlock(\n",
       "        (self_attn): Attention(\n",
       "          (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (rope): RoPE(128, traditional=False)\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "          (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "          (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "      )\n",
       "      (layers.1): TransformerBlock(\n",
       "        (self_attn): Attention(\n",
       "          (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (rope): RoPE(128, traditional=False)\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "          (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "          (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "      )\n",
       "      (layers.2): TransformerBlock(\n",
       "        (self_attn): Attention(\n",
       "          (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (rope): RoPE(128, traditional=False)\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "          (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "          (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "      )\n",
       "      (layers.3): TransformerBlock(\n",
       "        (self_attn): Attention(\n",
       "          (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (rope): RoPE(128, traditional=False)\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "          (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "          (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "      )\n",
       "      (layers.4): TransformerBlock(\n",
       "        (self_attn): Attention(\n",
       "          (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (rope): RoPE(128, traditional=False)\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "          (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "          (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "      )\n",
       "      (layers.5): TransformerBlock(\n",
       "        (self_attn): Attention(\n",
       "          (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (rope): RoPE(128, traditional=False)\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "          (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "          (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "      )\n",
       "      (layers.6): TransformerBlock(\n",
       "        (self_attn): Attention(\n",
       "          (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (rope): RoPE(128, traditional=False)\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "          (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "          (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "      )\n",
       "      (layers.7): TransformerBlock(\n",
       "        (self_attn): Attention(\n",
       "          (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (rope): RoPE(128, traditional=False)\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "          (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "          (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "      )\n",
       "      (layers.8): TransformerBlock(\n",
       "        (self_attn): Attention(\n",
       "          (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (rope): RoPE(128, traditional=False)\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "          (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "          (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "      )\n",
       "      (layers.9): TransformerBlock(\n",
       "        (self_attn): Attention(\n",
       "          (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (rope): RoPE(128, traditional=False)\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "          (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "          (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "      )\n",
       "      (layers.10): TransformerBlock(\n",
       "        (self_attn): Attention(\n",
       "          (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (rope): RoPE(128, traditional=False)\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "          (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "          (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "      )\n",
       "      (layers.11): TransformerBlock(\n",
       "        (self_attn): Attention(\n",
       "          (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (rope): RoPE(128, traditional=False)\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "          (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "          (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "      )\n",
       "      (layers.12): TransformerBlock(\n",
       "        (self_attn): Attention(\n",
       "          (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (rope): RoPE(128, traditional=False)\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "          (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "          (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "      )\n",
       "      (layers.13): TransformerBlock(\n",
       "        (self_attn): Attention(\n",
       "          (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (rope): RoPE(128, traditional=False)\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "          (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "          (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "      )\n",
       "      (layers.14): TransformerBlock(\n",
       "        (self_attn): Attention(\n",
       "          (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (rope): RoPE(128, traditional=False)\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "          (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "          (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "      )\n",
       "      (layers.15): TransformerBlock(\n",
       "        (self_attn): Attention(\n",
       "          (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (rope): RoPE(128, traditional=False)\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "          (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "          (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "      )\n",
       "      (layers.16): TransformerBlock(\n",
       "        (self_attn): Attention(\n",
       "          (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (rope): RoPE(128, traditional=False)\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "          (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "          (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "      )\n",
       "      (layers.17): TransformerBlock(\n",
       "        (self_attn): Attention(\n",
       "          (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (rope): RoPE(128, traditional=False)\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "          (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "          (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "      )\n",
       "      (layers.18): TransformerBlock(\n",
       "        (self_attn): Attention(\n",
       "          (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (rope): RoPE(128, traditional=False)\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "          (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "          (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "      )\n",
       "      (layers.19): TransformerBlock(\n",
       "        (self_attn): Attention(\n",
       "          (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (rope): RoPE(128, traditional=False)\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "          (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "          (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "      )\n",
       "      (layers.20): TransformerBlock(\n",
       "        (self_attn): Attention(\n",
       "          (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (rope): RoPE(128, traditional=False)\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "          (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "          (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "      )\n",
       "      (layers.21): TransformerBlock(\n",
       "        (self_attn): Attention(\n",
       "          (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (rope): RoPE(128, traditional=False)\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "          (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "          (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "      )\n",
       "      (layers.22): TransformerBlock(\n",
       "        (self_attn): Attention(\n",
       "          (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (rope): RoPE(128, traditional=False)\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "          (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "          (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "      )\n",
       "      (layers.23): TransformerBlock(\n",
       "        (self_attn): Attention(\n",
       "          (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (rope): RoPE(128, traditional=False)\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "          (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "          (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "      )\n",
       "      (layers.24): TransformerBlock(\n",
       "        (self_attn): Attention(\n",
       "          (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (rope): RoPE(128, traditional=False)\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "          (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "          (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "      )\n",
       "      (layers.25): TransformerBlock(\n",
       "        (self_attn): Attention(\n",
       "          (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (rope): RoPE(128, traditional=False)\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "          (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "          (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "      )\n",
       "      (layers.26): TransformerBlock(\n",
       "        (self_attn): Attention(\n",
       "          (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (rope): RoPE(128, traditional=False)\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "          (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "          (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "      )\n",
       "      (layers.27): TransformerBlock(\n",
       "        (self_attn): Attention(\n",
       "          (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (rope): RoPE(128, traditional=False)\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "          (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "          (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "      )\n",
       "      (layers.28): TransformerBlock(\n",
       "        (self_attn): Attention(\n",
       "          (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (rope): RoPE(128, traditional=False)\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "          (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "          (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "      )\n",
       "      (layers.29): TransformerBlock(\n",
       "        (self_attn): Attention(\n",
       "          (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (rope): RoPE(128, traditional=False)\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "          (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "          (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "      )\n",
       "      (layers.30): TransformerBlock(\n",
       "        (self_attn): Attention(\n",
       "          (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (rope): RoPE(128, traditional=False)\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "          (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "          (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "      )\n",
       "      (layers.31): TransformerBlock(\n",
       "        (self_attn): Attention(\n",
       "          (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "          (rope): RoPE(128, traditional=False)\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "          (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "          (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "      )\n",
       "      (norm): RMSNorm()\n",
       "    )\n",
       "    (lm_head): Linear(input_dims=4096, output_dims=32064, bias=False)\n",
       "  )\n",
       "  (multi_modal_projector): LlavaMultiModalProjector(\n",
       "    (linear_1): Linear(input_dims=1024, output_dims=4096, bias=True)\n",
       "    (gelu): GELU()\n",
       "    (linear_2): Linear(input_dims=4096, output_dims=4096, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlx_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Now that model weights are loaded in, now we can try and run inference code / set that up.\n",
    "\n",
    "# load the processor\n",
    "from transformers import AutoProcessor\n",
    "import requests\n",
    "from PIL import Image\n",
    "processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n",
    "\n",
    "prompt = \"<image>\\nUSER: What's the content of the image?\\nASSISTANT:\"\n",
    "url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_ids = mx.array(inputs[\"input_ids\"].numpy())\n",
    "pixel_values = mx.array(inputs[\"pixel_values\"].numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_model_output = mlx_model.vision_tower(pixel_values.transpose(0,2,3,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 577, 1024)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPVisionOutput(pooler_output=array([[-0.721487, -0.476275, 0.0173661, ..., 0.190072, -1.71528, 1.36224]], dtype=float32), last_hidden_state=array([[[-0.333623, -0.269844, 0.025435, ..., -0.0516554, -0.729696, 0.542679],\n",
       "        [0.208684, 0.92752, 0.0233985, ..., 1.59934, -0.024813, 0.879629],\n",
       "        [0.550235, 0.45201, 0.80935, ..., 1.63056, -0.37727, 0.699322],\n",
       "        ...,\n",
       "        [0.740987, 0.445616, 0.893172, ..., 0.523529, 0.0230118, -0.457155],\n",
       "        [0.49297, 0.0680847, 0.79401, ..., 0.476083, 0.274526, -0.284749],\n",
       "        [-0.0411091, 0.290756, 0.518906, ..., 0.242572, 0.40785, 0.420446]]], dtype=float32))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vision_model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
