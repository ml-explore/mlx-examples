{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal: Download and convert the weights of LlaVA into MLX, and test the forward pass of this model on example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlx_path = Path('mlx_model')\n",
    "\n",
    "if not os.path.exists(mlx_path):\n",
    "    os.makedirs(mlx_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/noahkasmanoff/anaconda3/envs/mlx/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 12 files: 100%|██████████| 12/12 [00:00<00:00, 202950.19it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import mlx.core as mx\n",
    "from convert import get_model_path, fetch_from_hub, hf_repo\n",
    "\n",
    "\n",
    "model_path = get_model_path(hf_repo)\n",
    "model_config, model_weights, model_weight_files, config, tokenizer = fetch_from_hub(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import map_weights, should_keep_weight\n",
    "do_convert = False\n",
    "if do_convert:\n",
    "\n",
    "    print(\"[INFO] Converting\")\n",
    "    mlx_weights = dict(map_weights(k, v) for (k, v) in model_weights.items())\n",
    "    mlx_weights = {k: v for (k, v) in mlx_weights.items() if should_keep_weight(k)}\n",
    "    print(\"[INFO] Saving\")\n",
    "    mx.savez(str(mlx_path / \"weights.npz\"), **mlx_weights)\n",
    "    for fn in [\"config.json\", \"merges.txt\", \"vocab.json\", \"preprocessor_config.json\"]:\n",
    "        if fn in os.listdir(model_path):\n",
    "            shutil.copyfile(\n",
    "                str(model_path / f\"{fn}\"),\n",
    "                str(mlx_path / f\"{fn}\"),\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llava import LlaVAConfig, LLMConfig, VisionConfig, ProjectionConfig, LlavaModel\n",
    "\n",
    "llava_mlx_config = LlaVAConfig(\n",
    "    llm_config=LLMConfig(\n",
    "        model_type='vicuna',\n",
    "        hidden_size=4096,\n",
    "        num_hidden_layers=32,\n",
    "        intermediate_size=11008,\n",
    "        num_attention_heads=32,\n",
    "        rms_norm_eps=1e-5,\n",
    "        vocab_size=32064,\n",
    "        num_key_value_heads=32,\n",
    "        rope_theta=0,\n",
    "        rope_traditional=False,\n",
    "        rope_scaling=None\n",
    "        ),\n",
    "    vision_config=VisionConfig(\n",
    "        num_hidden_layers=24,\n",
    "        hidden_size=1024,\n",
    "        intermediate_size=4096,\n",
    "        num_attention_heads=16,\n",
    "        num_channels=3,\n",
    "        image_size=336,\n",
    "        patch_size=14\n",
    "    ),\n",
    "    projection_config=ProjectionConfig(\n",
    "        in_features=1024,\n",
    "        out_features=4096\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "mlx_model = LlavaModel(llava_mlx_config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.6875"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "11008 / 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Received parameters not in model: language_model.layers.20.feed_forward.w3.weight language_model.layers.18.feed_forward.w2.weight language_model.layers.20.feed_forward.w1.weight language_model.layers.0.feed_forward.w1.weight language_model.layers.23.attention.wq.weight language_model.layers.23.ffn_norm.weight language_model.layers.16.feed_forward.w1.weight language_model.layers.31.attention_norm.weight language_model.output.weight language_model.layers.27.attention_norm.weight language_model.layers.25.attention_norm.weight language_model.layers.28.attention_norm.weight language_model.layers.20.feed_forward.w2.weight language_model.layers.17.attention.wo.weight language_model.layers.1.attention.wq.weight language_model.layers.27.feed_forward.w3.weight language_model.layers.19.feed_forward.w1.weight language_model.layers.14.attention_norm.weight language_model.layers.21.feed_forward.w2.weight language_model.layers.16.attention.wo.weight language_model.layers.22.attention_norm.weight language_model.layers.4.attention.wk.weight language_model.layers.13.feed_forward.w1.weight language_model.layers.30.attention.wv.weight language_model.layers.5.feed_forward.w3.weight language_model.layers.20.attention_norm.weight language_model.layers.13.feed_forward.w2.weight language_model.layers.22.feed_forward.w2.weight language_model.layers.15.attention.wo.weight language_model.layers.26.attention.wo.weight language_model.layers.5.feed_forward.w1.weight language_model.layers.16.attention_norm.weight language_model.layers.4.attention.wq.weight language_model.layers.9.feed_forward.w1.weight language_model.layers.20.attention.wq.weight language_model.layers.9.attention.wv.weight language_model.layers.10.ffn_norm.weight language_model.layers.8.attention.wo.weight language_model.layers.3.attention.wv.weight language_model.layers.0.ffn_norm.weight language_model.layers.4.feed_forward.w3.weight language_model.layers.2.attention.wv.weight language_model.layers.7.attention.wv.weight language_model.layers.24.attention.wq.weight language_model.layers.11.feed_forward.w2.weight language_model.layers.0.attention.wo.weight language_model.layers.7.feed_forward.w3.weight language_model.layers.17.feed_forward.w1.weight language_model.layers.31.attention.wo.weight language_model.layers.26.attention.wk.weight language_model.layers.0.feed_forward.w3.weight language_model.layers.2.ffn_norm.weight language_model.layers.13.attention_norm.weight language_model.layers.19.attention.wk.weight language_model.layers.18.attention.wq.weight language_model.layers.10.attention.wo.weight language_model.layers.30.attention.wq.weight language_model.layers.5.feed_forward.w2.weight language_model.layers.5.attention.wv.weight language_model.layers.25.attention.wq.weight language_model.layers.3.feed_forward.w3.weight language_model.layers.9.attention.wo.weight language_model.layers.29.feed_forward.w1.weight language_model.layers.2.feed_forward.w3.weight language_model.layers.0.attention.wk.weight language_model.layers.11.attention.wv.weight language_model.layers.20.attention.wo.weight language_model.layers.16.ffn_norm.weight language_model.layers.2.feed_forward.w2.weight language_model.layers.27.attention.wk.weight language_model.tok_embeddings.weight language_model.layers.14.ffn_norm.weight language_model.layers.12.ffn_norm.weight language_model.layers.22.attention.wo.weight language_model.layers.12.attention.wq.weight language_model.layers.19.attention.wq.weight language_model.layers.11.attention.wq.weight language_model.layers.6.attention.wv.weight language_model.layers.26.feed_forward.w3.weight language_model.layers.26.feed_forward.w2.weight language_model.layers.17.attention.wq.weight language_model.layers.18.feed_forward.w3.weight language_model.layers.29.attention.wk.weight language_model.layers.29.feed_forward.w2.weight language_model.layers.8.attention.wq.weight language_model.layers.2.attention_norm.weight language_model.layers.5.attention.wo.weight language_model.layers.23.feed_forward.w1.weight language_model.layers.30.feed_forward.w1.weight language_model.layers.2.attention.wo.weight language_model.layers.18.attention.wk.weight language_model.layers.13.attention.wo.weight language_model.layers.3.ffn_norm.weight language_model.layers.23.attention_norm.weight language_model.layers.10.feed_forward.w2.weight language_model.layers.1.ffn_norm.weight language_model.layers.21.ffn_norm.weight language_model.layers.30.attention.wo.weight language_model.layers.11.attention.wk.weight language_model.layers.7.attention.wo.weight language_model.layers.17.feed_forward.w3.weight language_model.layers.13.ffn_norm.weight language_model.layers.3.feed_forward.w1.weight language_model.layers.18.attention.wo.weight language_model.layers.22.feed_forward.w1.weight language_model.layers.15.attention.wk.weight language_model.layers.15.attention.wv.weight language_model.layers.28.feed_forward.w2.weight language_model.layers.21.feed_forward.w1.weight language_model.layers.12.feed_forward.w2.weight language_model.layers.23.feed_forward.w3.weight language_model.layers.19.ffn_norm.weight language_model.layers.18.attention_norm.weight language_model.layers.22.feed_forward.w3.weight language_model.layers.14.attention.wo.weight language_model.layers.9.ffn_norm.weight language_model.layers.13.attention.wk.weight language_model.layers.28.attention.wo.weight language_model.layers.26.attention.wq.weight language_model.layers.24.ffn_norm.weight language_model.layers.23.attention.wo.weight language_model.layers.10.attention_norm.weight language_model.layers.16.feed_forward.w2.weight language_model.layers.19.feed_forward.w2.weight language_model.layers.23.attention.wk.weight language_model.layers.2.feed_forward.w1.weight language_model.layers.11.feed_forward.w1.weight language_model.layers.4.feed_forward.w2.weight language_model.layers.23.attention.wv.weight language_model.layers.27.feed_forward.w1.weight language_model.layers.17.feed_forward.w2.weight language_model.layers.12.attention_norm.weight language_model.layers.30.feed_forward.w2.weight language_model.layers.15.ffn_norm.weight language_model.layers.12.feed_forward.w1.weight language_model.layers.28.attention.wq.weight language_model.layers.10.attention.wq.weight language_model.layers.4.ffn_norm.weight language_model.layers.14.feed_forward.w1.weight language_model.layers.3.feed_forward.w2.weight language_model.layers.12.feed_forward.w3.weight language_model.layers.21.attention.wq.weight language_model.layers.10.attention.wv.weight language_model.layers.21.feed_forward.w3.weight language_model.layers.6.feed_forward.w2.weight language_model.layers.20.ffn_norm.weight language_model.layers.25.attention.wk.weight language_model.layers.17.attention.wk.weight language_model.layers.26.attention_norm.weight language_model.layers.25.feed_forward.w2.weight language_model.layers.1.attention_norm.weight language_model.layers.26.attention.wv.weight language_model.layers.19.attention.wo.weight language_model.layers.14.feed_forward.w3.weight language_model.layers.14.attention.wv.weight language_model.layers.29.ffn_norm.weight language_model.layers.14.feed_forward.w2.weight language_model.layers.1.attention.wk.weight language_model.layers.4.attention.wv.weight language_model.layers.22.attention.wq.weight language_model.layers.3.attention.wq.weight language_model.layers.16.attention.wv.weight language_model.layers.21.attention.wo.weight language_model.layers.26.ffn_norm.weight language_model.layers.29.attention.wq.weight language_model.layers.7.attention.wq.weight language_model.layers.21.attention_norm.weight language_model.layers.24.attention.wo.weight language_model.layers.5.attention_norm.weight language_model.layers.18.feed_forward.w1.weight language_model.layers.26.feed_forward.w1.weight language_model.layers.31.attention.wv.weight language_model.layers.25.feed_forward.w1.weight language_model.layers.27.ffn_norm.weight language_model.layers.6.feed_forward.w1.weight language_model.layers.28.feed_forward.w1.weight language_model.layers.1.feed_forward.w3.weight language_model.layers.8.feed_forward.w2.weight language_model.layers.20.attention.wk.weight language_model.layers.2.attention.wq.weight language_model.layers.4.feed_forward.w1.weight language_model.layers.9.attention.wq.weight language_model.layers.15.feed_forward.w1.weight language_model.layers.7.ffn_norm.weight language_model.layers.0.feed_forward.w2.weight language_model.layers.30.attention_norm.weight language_model.layers.13.attention.wv.weight language_model.layers.10.feed_forward.w1.weight language_model.layers.5.attention.wq.weight language_model.layers.16.feed_forward.w3.weight language_model.layers.28.ffn_norm.weight language_model.layers.31.feed_forward.w1.weight language_model.layers.12.attention.wo.weight language_model.layers.27.attention.wo.weight language_model.layers.15.feed_forward.w3.weight language_model.layers.29.attention.wo.weight language_model.layers.27.attention.wv.weight language_model.layers.14.attention.wq.weight language_model.layers.5.attention.wk.weight language_model.layers.1.feed_forward.w1.weight language_model.layers.20.attention.wv.weight language_model.layers.23.feed_forward.w2.weight language_model.layers.8.attention.wk.weight language_model.layers.5.ffn_norm.weight language_model.layers.21.attention.wv.weight language_model.layers.29.attention_norm.weight language_model.layers.10.feed_forward.w3.weight language_model.layers.1.feed_forward.w2.weight language_model.layers.24.feed_forward.w3.weight language_model.layers.11.ffn_norm.weight language_model.layers.9.attention_norm.weight language_model.layers.4.attention.wo.weight language_model.layers.25.attention.wo.weight language_model.layers.7.feed_forward.w2.weight language_model.layers.9.feed_forward.w2.weight language_model.layers.14.attention.wk.weight language_model.layers.27.feed_forward.w2.weight language_model.layers.13.attention.wq.weight language_model.layers.15.attention_norm.weight language_model.layers.28.attention.wv.weight language_model.layers.0.attention_norm.weight language_model.layers.0.attention.wv.weight language_model.layers.7.attention.wk.weight language_model.layers.29.feed_forward.w3.weight language_model.layers.3.attention.wk.weight language_model.layers.28.feed_forward.w3.weight language_model.layers.22.attention.wv.weight language_model.layers.22.attention.wk.weight language_model.layers.6.attention.wq.weight language_model.layers.1.attention.wo.weight language_model.layers.18.attention.wv.weight language_model.layers.8.attention.wv.weight language_model.layers.6.ffn_norm.weight language_model.layers.25.ffn_norm.weight language_model.layers.8.attention_norm.weight language_model.layers.6.attention.wk.weight language_model.layers.29.attention.wv.weight language_model.layers.19.attention_norm.weight language_model.layers.19.attention.wv.weight language_model.layers.6.attention.wo.weight language_model.layers.12.attention.wk.weight language_model.layers.9.feed_forward.w3.weight language_model.layers.8.feed_forward.w1.weight language_model.layers.10.attention.wk.weight language_model.layers.17.ffn_norm.weight language_model.layers.21.attention.wk.weight language_model.layers.15.attention.wq.weight language_model.layers.11.attention_norm.weight language_model.layers.24.attention.wk.weight language_model.layers.31.feed_forward.w2.weight language_model.layers.18.ffn_norm.weight language_model.layers.30.feed_forward.w3.weight language_model.layers.22.ffn_norm.weight language_model.layers.28.attention.wk.weight language_model.layers.9.attention.wk.weight language_model.layers.24.feed_forward.w2.weight language_model.layers.17.attention_norm.weight language_model.layers.17.attention.wv.weight language_model.layers.1.attention.wv.weight language_model.layers.31.ffn_norm.weight language_model.layers.31.attention.wk.weight language_model.layers.24.feed_forward.w1.weight language_model.layers.8.feed_forward.w3.weight language_model.layers.25.attention.wv.weight language_model.layers.7.feed_forward.w1.weight language_model.layers.31.attention.wq.weight language_model.layers.15.feed_forward.w2.weight language_model.layers.30.ffn_norm.weight language_model.layers.0.attention.wq.weight language_model.layers.31.feed_forward.w3.weight language_model.layers.13.feed_forward.w3.weight language_model.layers.19.feed_forward.w3.weight language_model.layers.6.attention_norm.weight language_model.layers.4.attention_norm.weight language_model.layers.12.attention.wv.weight language_model.layers.8.ffn_norm.weight language_model.layers.30.attention.wk.weight language_model.layers.3.attention.wo.weight language_model.layers.16.attention.wq.weight language_model.layers.11.feed_forward.w3.weight language_model.layers.25.feed_forward.w3.weight language_model.layers.3.attention_norm.weight language_model.layers.2.attention.wk.weight language_model.layers.16.attention.wk.weight language_model.layers.7.attention_norm.weight language_model.layers.27.attention.wq.weight language_model.layers.6.feed_forward.w3.weight language_model.layers.24.attention.wv.weight language_model.layers.11.attention.wo.weight language_model.layers.24.attention_norm.weight.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmlx_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmlx_model/weights.npz\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mlx/lib/python3.10/site-packages/mlx/nn/layers/base.py:164\u001b[0m, in \u001b[0;36mModule.load_weights\u001b[0;34m(self, file_or_weights, strict)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extras \u001b[38;5;241m:=\u001b[39m (new_weights\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;241m-\u001b[39m curr_weights\u001b[38;5;241m.\u001b[39mkeys()):\n\u001b[1;32m    163\u001b[0m     extras \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(extras)\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived parameters not in model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextras\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing \u001b[38;5;241m:=\u001b[39m (curr_weights\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;241m-\u001b[39m new_weights\u001b[38;5;241m.\u001b[39mkeys()):\n\u001b[1;32m    166\u001b[0m     missing \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(missing)\n",
      "\u001b[0;31mValueError\u001b[0m: Received parameters not in model: language_model.layers.20.feed_forward.w3.weight language_model.layers.18.feed_forward.w2.weight language_model.layers.20.feed_forward.w1.weight language_model.layers.0.feed_forward.w1.weight language_model.layers.23.attention.wq.weight language_model.layers.23.ffn_norm.weight language_model.layers.16.feed_forward.w1.weight language_model.layers.31.attention_norm.weight language_model.output.weight language_model.layers.27.attention_norm.weight language_model.layers.25.attention_norm.weight language_model.layers.28.attention_norm.weight language_model.layers.20.feed_forward.w2.weight language_model.layers.17.attention.wo.weight language_model.layers.1.attention.wq.weight language_model.layers.27.feed_forward.w3.weight language_model.layers.19.feed_forward.w1.weight language_model.layers.14.attention_norm.weight language_model.layers.21.feed_forward.w2.weight language_model.layers.16.attention.wo.weight language_model.layers.22.attention_norm.weight language_model.layers.4.attention.wk.weight language_model.layers.13.feed_forward.w1.weight language_model.layers.30.attention.wv.weight language_model.layers.5.feed_forward.w3.weight language_model.layers.20.attention_norm.weight language_model.layers.13.feed_forward.w2.weight language_model.layers.22.feed_forward.w2.weight language_model.layers.15.attention.wo.weight language_model.layers.26.attention.wo.weight language_model.layers.5.feed_forward.w1.weight language_model.layers.16.attention_norm.weight language_model.layers.4.attention.wq.weight language_model.layers.9.feed_forward.w1.weight language_model.layers.20.attention.wq.weight language_model.layers.9.attention.wv.weight language_model.layers.10.ffn_norm.weight language_model.layers.8.attention.wo.weight language_model.layers.3.attention.wv.weight language_model.layers.0.ffn_norm.weight language_model.layers.4.feed_forward.w3.weight language_model.layers.2.attention.wv.weight language_model.layers.7.attention.wv.weight language_model.layers.24.attention.wq.weight language_model.layers.11.feed_forward.w2.weight language_model.layers.0.attention.wo.weight language_model.layers.7.feed_forward.w3.weight language_model.layers.17.feed_forward.w1.weight language_model.layers.31.attention.wo.weight language_model.layers.26.attention.wk.weight language_model.layers.0.feed_forward.w3.weight language_model.layers.2.ffn_norm.weight language_model.layers.13.attention_norm.weight language_model.layers.19.attention.wk.weight language_model.layers.18.attention.wq.weight language_model.layers.10.attention.wo.weight language_model.layers.30.attention.wq.weight language_model.layers.5.feed_forward.w2.weight language_model.layers.5.attention.wv.weight language_model.layers.25.attention.wq.weight language_model.layers.3.feed_forward.w3.weight language_model.layers.9.attention.wo.weight language_model.layers.29.feed_forward.w1.weight language_model.layers.2.feed_forward.w3.weight language_model.layers.0.attention.wk.weight language_model.layers.11.attention.wv.weight language_model.layers.20.attention.wo.weight language_model.layers.16.ffn_norm.weight language_model.layers.2.feed_forward.w2.weight language_model.layers.27.attention.wk.weight language_model.tok_embeddings.weight language_model.layers.14.ffn_norm.weight language_model.layers.12.ffn_norm.weight language_model.layers.22.attention.wo.weight language_model.layers.12.attention.wq.weight language_model.layers.19.attention.wq.weight language_model.layers.11.attention.wq.weight language_model.layers.6.attention.wv.weight language_model.layers.26.feed_forward.w3.weight language_model.layers.26.feed_forward.w2.weight language_model.layers.17.attention.wq.weight language_model.layers.18.feed_forward.w3.weight language_model.layers.29.attention.wk.weight language_model.layers.29.feed_forward.w2.weight language_model.layers.8.attention.wq.weight language_model.layers.2.attention_norm.weight language_model.layers.5.attention.wo.weight language_model.layers.23.feed_forward.w1.weight language_model.layers.30.feed_forward.w1.weight language_model.layers.2.attention.wo.weight language_model.layers.18.attention.wk.weight language_model.layers.13.attention.wo.weight language_model.layers.3.ffn_norm.weight language_model.layers.23.attention_norm.weight language_model.layers.10.feed_forward.w2.weight language_model.layers.1.ffn_norm.weight language_model.layers.21.ffn_norm.weight language_model.layers.30.attention.wo.weight language_model.layers.11.attention.wk.weight language_model.layers.7.attention.wo.weight language_model.layers.17.feed_forward.w3.weight language_model.layers.13.ffn_norm.weight language_model.layers.3.feed_forward.w1.weight language_model.layers.18.attention.wo.weight language_model.layers.22.feed_forward.w1.weight language_model.layers.15.attention.wk.weight language_model.layers.15.attention.wv.weight language_model.layers.28.feed_forward.w2.weight language_model.layers.21.feed_forward.w1.weight language_model.layers.12.feed_forward.w2.weight language_model.layers.23.feed_forward.w3.weight language_model.layers.19.ffn_norm.weight language_model.layers.18.attention_norm.weight language_model.layers.22.feed_forward.w3.weight language_model.layers.14.attention.wo.weight language_model.layers.9.ffn_norm.weight language_model.layers.13.attention.wk.weight language_model.layers.28.attention.wo.weight language_model.layers.26.attention.wq.weight language_model.layers.24.ffn_norm.weight language_model.layers.23.attention.wo.weight language_model.layers.10.attention_norm.weight language_model.layers.16.feed_forward.w2.weight language_model.layers.19.feed_forward.w2.weight language_model.layers.23.attention.wk.weight language_model.layers.2.feed_forward.w1.weight language_model.layers.11.feed_forward.w1.weight language_model.layers.4.feed_forward.w2.weight language_model.layers.23.attention.wv.weight language_model.layers.27.feed_forward.w1.weight language_model.layers.17.feed_forward.w2.weight language_model.layers.12.attention_norm.weight language_model.layers.30.feed_forward.w2.weight language_model.layers.15.ffn_norm.weight language_model.layers.12.feed_forward.w1.weight language_model.layers.28.attention.wq.weight language_model.layers.10.attention.wq.weight language_model.layers.4.ffn_norm.weight language_model.layers.14.feed_forward.w1.weight language_model.layers.3.feed_forward.w2.weight language_model.layers.12.feed_forward.w3.weight language_model.layers.21.attention.wq.weight language_model.layers.10.attention.wv.weight language_model.layers.21.feed_forward.w3.weight language_model.layers.6.feed_forward.w2.weight language_model.layers.20.ffn_norm.weight language_model.layers.25.attention.wk.weight language_model.layers.17.attention.wk.weight language_model.layers.26.attention_norm.weight language_model.layers.25.feed_forward.w2.weight language_model.layers.1.attention_norm.weight language_model.layers.26.attention.wv.weight language_model.layers.19.attention.wo.weight language_model.layers.14.feed_forward.w3.weight language_model.layers.14.attention.wv.weight language_model.layers.29.ffn_norm.weight language_model.layers.14.feed_forward.w2.weight language_model.layers.1.attention.wk.weight language_model.layers.4.attention.wv.weight language_model.layers.22.attention.wq.weight language_model.layers.3.attention.wq.weight language_model.layers.16.attention.wv.weight language_model.layers.21.attention.wo.weight language_model.layers.26.ffn_norm.weight language_model.layers.29.attention.wq.weight language_model.layers.7.attention.wq.weight language_model.layers.21.attention_norm.weight language_model.layers.24.attention.wo.weight language_model.layers.5.attention_norm.weight language_model.layers.18.feed_forward.w1.weight language_model.layers.26.feed_forward.w1.weight language_model.layers.31.attention.wv.weight language_model.layers.25.feed_forward.w1.weight language_model.layers.27.ffn_norm.weight language_model.layers.6.feed_forward.w1.weight language_model.layers.28.feed_forward.w1.weight language_model.layers.1.feed_forward.w3.weight language_model.layers.8.feed_forward.w2.weight language_model.layers.20.attention.wk.weight language_model.layers.2.attention.wq.weight language_model.layers.4.feed_forward.w1.weight language_model.layers.9.attention.wq.weight language_model.layers.15.feed_forward.w1.weight language_model.layers.7.ffn_norm.weight language_model.layers.0.feed_forward.w2.weight language_model.layers.30.attention_norm.weight language_model.layers.13.attention.wv.weight language_model.layers.10.feed_forward.w1.weight language_model.layers.5.attention.wq.weight language_model.layers.16.feed_forward.w3.weight language_model.layers.28.ffn_norm.weight language_model.layers.31.feed_forward.w1.weight language_model.layers.12.attention.wo.weight language_model.layers.27.attention.wo.weight language_model.layers.15.feed_forward.w3.weight language_model.layers.29.attention.wo.weight language_model.layers.27.attention.wv.weight language_model.layers.14.attention.wq.weight language_model.layers.5.attention.wk.weight language_model.layers.1.feed_forward.w1.weight language_model.layers.20.attention.wv.weight language_model.layers.23.feed_forward.w2.weight language_model.layers.8.attention.wk.weight language_model.layers.5.ffn_norm.weight language_model.layers.21.attention.wv.weight language_model.layers.29.attention_norm.weight language_model.layers.10.feed_forward.w3.weight language_model.layers.1.feed_forward.w2.weight language_model.layers.24.feed_forward.w3.weight language_model.layers.11.ffn_norm.weight language_model.layers.9.attention_norm.weight language_model.layers.4.attention.wo.weight language_model.layers.25.attention.wo.weight language_model.layers.7.feed_forward.w2.weight language_model.layers.9.feed_forward.w2.weight language_model.layers.14.attention.wk.weight language_model.layers.27.feed_forward.w2.weight language_model.layers.13.attention.wq.weight language_model.layers.15.attention_norm.weight language_model.layers.28.attention.wv.weight language_model.layers.0.attention_norm.weight language_model.layers.0.attention.wv.weight language_model.layers.7.attention.wk.weight language_model.layers.29.feed_forward.w3.weight language_model.layers.3.attention.wk.weight language_model.layers.28.feed_forward.w3.weight language_model.layers.22.attention.wv.weight language_model.layers.22.attention.wk.weight language_model.layers.6.attention.wq.weight language_model.layers.1.attention.wo.weight language_model.layers.18.attention.wv.weight language_model.layers.8.attention.wv.weight language_model.layers.6.ffn_norm.weight language_model.layers.25.ffn_norm.weight language_model.layers.8.attention_norm.weight language_model.layers.6.attention.wk.weight language_model.layers.29.attention.wv.weight language_model.layers.19.attention_norm.weight language_model.layers.19.attention.wv.weight language_model.layers.6.attention.wo.weight language_model.layers.12.attention.wk.weight language_model.layers.9.feed_forward.w3.weight language_model.layers.8.feed_forward.w1.weight language_model.layers.10.attention.wk.weight language_model.layers.17.ffn_norm.weight language_model.layers.21.attention.wk.weight language_model.layers.15.attention.wq.weight language_model.layers.11.attention_norm.weight language_model.layers.24.attention.wk.weight language_model.layers.31.feed_forward.w2.weight language_model.layers.18.ffn_norm.weight language_model.layers.30.feed_forward.w3.weight language_model.layers.22.ffn_norm.weight language_model.layers.28.attention.wk.weight language_model.layers.9.attention.wk.weight language_model.layers.24.feed_forward.w2.weight language_model.layers.17.attention_norm.weight language_model.layers.17.attention.wv.weight language_model.layers.1.attention.wv.weight language_model.layers.31.ffn_norm.weight language_model.layers.31.attention.wk.weight language_model.layers.24.feed_forward.w1.weight language_model.layers.8.feed_forward.w3.weight language_model.layers.25.attention.wv.weight language_model.layers.7.feed_forward.w1.weight language_model.layers.31.attention.wq.weight language_model.layers.15.feed_forward.w2.weight language_model.layers.30.ffn_norm.weight language_model.layers.0.attention.wq.weight language_model.layers.31.feed_forward.w3.weight language_model.layers.13.feed_forward.w3.weight language_model.layers.19.feed_forward.w3.weight language_model.layers.6.attention_norm.weight language_model.layers.4.attention_norm.weight language_model.layers.12.attention.wv.weight language_model.layers.8.ffn_norm.weight language_model.layers.30.attention.wk.weight language_model.layers.3.attention.wo.weight language_model.layers.16.attention.wq.weight language_model.layers.11.feed_forward.w3.weight language_model.layers.25.feed_forward.w3.weight language_model.layers.3.attention_norm.weight language_model.layers.2.attention.wk.weight language_model.layers.16.attention.wk.weight language_model.layers.7.attention_norm.weight language_model.layers.27.attention.wq.weight language_model.layers.6.feed_forward.w3.weight language_model.layers.24.attention.wv.weight language_model.layers.11.attention.wo.weight language_model.layers.24.attention_norm.weight."
     ]
    }
   ],
   "source": [
    "mlx_model.load_weights('mlx_model/weights.npz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: load images, and test generate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:07<00:00,  2.48s/it]\n"
     ]
    }
   ],
   "source": [
    "# TODO: compare with hf version's model weights as well \n",
    "\n",
    "# Load model directly\n",
    "from transformers import AutoProcessor, AutoModelForPreTraining\n",
    "processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n",
    "model = AutoModelForPreTraining.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00692749, -0.0147705, -0.00254822, ..., 0.00500488, 0.00238037, -0.0027771],\n",
       "       [0.0155029, -0.00343323, 0.00121307, ..., -0.00964355, -0.0110474, 0.00744629],\n",
       "       [-0.0157471, 0.0144043, 0.000104904, ..., 0.00619507, 0.0189209, -0.00415039],\n",
       "       ...,\n",
       "       [1.54972e-06, 0.00866699, 0.000881195, ..., 0.00946045, -0.0301514, 0.0107422],\n",
       "       [0.0253906, 0.00994873, 0.00454712, ..., -0.0319824, -0.0148926, -0.0130005],\n",
       "       [-0.0108643, -0.00534058, 0.00102234, ..., 0.0164795, 0.0150146, -0.00811768]], dtype=float16)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlx_model.language_model.layers[0].attention.wq.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-6.9275e-03, -1.4771e-02, -2.5482e-03,  ...,  5.0049e-03,\n",
       "          2.3804e-03, -2.7771e-03],\n",
       "        [ 1.5503e-02, -3.4332e-03,  1.2131e-03,  ..., -9.6436e-03,\n",
       "         -1.1047e-02,  7.4463e-03],\n",
       "        [-1.5747e-02,  1.4404e-02,  1.0490e-04,  ...,  6.1951e-03,\n",
       "          1.8921e-02, -4.1504e-03],\n",
       "        ...,\n",
       "        [ 1.5497e-06,  8.6670e-03,  8.8120e-04,  ...,  9.4604e-03,\n",
       "         -3.0151e-02,  1.0742e-02],\n",
       "        [ 2.5391e-02,  9.9487e-03,  4.5471e-03,  ..., -3.1982e-02,\n",
       "         -1.4893e-02, -1.3000e-02],\n",
       "        [-1.0864e-02, -5.3406e-03,  1.0223e-03,  ...,  1.6479e-02,\n",
       "          1.5015e-02, -8.1177e-03]], requires_grad=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.language_model.model.layers[0].self_attn.q_proj.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# They seem to be the same!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "image = Image.open(requests.get(\"https://llava-vl.github.io/static/images/view.jpg\", stream=True).raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = ['USER: <image> What are the things I should think aboutwhen I visit this place? ASSISTANT:'\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(prompts, images=[image], padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,  3148,  1001, 29901, 29871, 32000, 29871,  1724,   526,   278,\n",
       "          2712,   306,   881,  1348,  1048,  8256,   306,  6493,   445,  2058,\n",
       "         29973,   319,  1799,  9047, 13566, 29901]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1]]), 'pixel_values': tensor([[[[ 1.2734,  1.2734,  1.2734,  ...,  1.1274,  1.1274,  1.0982],\n",
       "          [ 1.2734,  1.2734,  1.2880,  ...,  1.1274,  1.1274,  1.1128],\n",
       "          [ 1.2880,  1.2880,  1.2880,  ...,  1.1274,  1.1274,  1.1274],\n",
       "          ...,\n",
       "          [-0.9456, -0.9164, -0.9164,  ..., -1.0769, -1.0769, -1.0769],\n",
       "          [-0.9602, -0.9310, -0.9018,  ..., -1.0915, -1.0915, -1.0915],\n",
       "          [-0.9602, -0.9748, -0.2448,  ..., -1.1061, -1.1061, -1.1207]],\n",
       "\n",
       "         [[ 1.6397,  1.6397,  1.6397,  ...,  1.5196,  1.5196,  1.5196],\n",
       "          [ 1.6397,  1.6397,  1.6547,  ...,  1.5196,  1.5196,  1.5196],\n",
       "          [ 1.6547,  1.6547,  1.6547,  ...,  1.5196,  1.5196,  1.5196],\n",
       "          ...,\n",
       "          [-0.5065, -0.5065, -0.5215,  ..., -0.6715, -0.6715, -0.6715],\n",
       "          [-0.5215, -0.5215, -0.5065,  ..., -0.6865, -0.6865, -0.6865],\n",
       "          [-0.5215, -0.5665,  0.1689,  ..., -0.7016, -0.7016, -0.7166]],\n",
       "\n",
       "         [[ 1.9610,  1.9610,  1.9610,  ...,  1.9042,  1.9042,  1.8899],\n",
       "          [ 1.9610,  1.9610,  1.9753,  ...,  1.9042,  1.9042,  1.8899],\n",
       "          [ 1.9753,  1.9753,  1.9753,  ...,  1.9042,  1.9042,  1.9042],\n",
       "          ...,\n",
       "          [-0.1009, -0.0724, -0.0867,  ..., -0.2573, -0.2573, -0.2573],\n",
       "          [-0.1009, -0.1009, -0.0867,  ..., -0.2715, -0.2715, -0.2715],\n",
       "          [-0.1009, -0.1578,  0.5390,  ..., -0.2857, -0.2857, -0.3000]]]])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlavaConfig {\n",
       "  \"_name_or_path\": \"/Users/noahkasmanoff/.cache/huggingface/hub/models--llava-hf--llava-1.5-7b-hf/snapshots/05ae2434cbb430be33edcba0c5203e7023f785b7\",\n",
       "  \"architectures\": [\n",
       "    \"LlavaForConditionalGeneration\"\n",
       "  ],\n",
       "  \"ignore_index\": -100,\n",
       "  \"image_token_index\": 32000,\n",
       "  \"model_type\": \"llava\",\n",
       "  \"pad_token_id\": 32001,\n",
       "  \"projector_hidden_act\": \"gelu\",\n",
       "  \"text_config\": {\n",
       "    \"_name_or_path\": \"lmsys/vicuna-7b-v1.5\",\n",
       "    \"architectures\": [\n",
       "      \"LlamaForCausalLM\"\n",
       "    ],\n",
       "    \"max_position_embeddings\": 4096,\n",
       "    \"model_type\": \"llama\",\n",
       "    \"rms_norm_eps\": 1e-05,\n",
       "    \"torch_dtype\": \"float16\",\n",
       "    \"vocab_size\": 32064\n",
       "  },\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"float16\",\n",
       "  \"transformers_version\": \"4.37.2\",\n",
       "  \"vision_config\": {\n",
       "    \"hidden_size\": 1024,\n",
       "    \"image_size\": 336,\n",
       "    \"intermediate_size\": 4096,\n",
       "    \"model_type\": \"clip_vision_model\",\n",
       "    \"num_attention_heads\": 16,\n",
       "    \"num_hidden_layers\": 24,\n",
       "    \"patch_size\": 14,\n",
       "    \"projection_dim\": 768,\n",
       "    \"vocab_size\": 32000\n",
       "  },\n",
       "  \"vision_feature_layer\": -2,\n",
       "  \"vision_feature_select_strategy\": \"default\",\n",
       "  \"vocab_size\": 32064\n",
       "}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
